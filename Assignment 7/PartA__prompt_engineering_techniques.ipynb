{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WdR16ZtLtQjc"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_community --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC3x6GWh1yhm",
        "outputId": "2f1c99bd-f16a-4d8a-dfaa-ba1ce3487ae4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.56.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from langchain.llms import OpenAI\n",
        "import os"
      ],
      "metadata": {
        "id": "zp4Krr561zkQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import OpenAI as LangChainOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from typing import List\n",
        "import json"
      ],
      "metadata": {
        "id": "toV5V0bPvgpT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "open_ai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "hugging_face_token = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "XkFuxGv9trLM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = open_ai_api_key"
      ],
      "metadata": {
        "id": "Y7hEMjEYtsVl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ],
      "metadata": {
        "id": "TksJf-161J31"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=open_ai_api_key)\n",
        "\n",
        "# Initialize LangChain LLM\n",
        "llm = LangChainOpenAI(temperature=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG09athvvZj0",
        "outputId": "7b637413-f55f-48ee-8dfd-60c9d82da7c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-9f17bca48abe>:2: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
            "  client = OpenAI(api_key=open_ai_api_key)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. In-Context Learning (ICL)**\n",
        "Success Case:"
      ],
      "metadata": {
        "id": "hbcvqMEIt9fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of ICL\n",
        "prompt = (\n",
        "    \"Translate the following sentences into French:\\n\"\n",
        "    \"1. I love programming. -> J'adore la programmation.\\n\"\n",
        "    \"2. How are you? -> Comment ça va?\\n\"\n",
        "    \"3. This is a test. ->\"\n",
        ")\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "response = openai.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    max_tokens=50,\n",
        "    temperature=0\n",
        ")\n",
        "print(\"Success Case:\", response.choices[0].message.content.strip())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OpbpCBst7IP",
        "outputId": "a698b8f5-9660-4534-f0d0-445f11f187c7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success Case: C'est un test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Failure Case:"
      ],
      "metadata": {
        "id": "M1qozOhI2usL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of ICL Failure (ambiguous examples or insufficient context)\n",
        "prompt = (\n",
        "    \"Translate to French:\\n\"\n",
        "    \"1. Hello -> Bonjour\\n\"\n",
        "    \"2. Bye -> Au revoir\\n\"\n",
        "    \"3. How do you do? ->\"\n",
        ")\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "response = openai.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    max_tokens=50,\n",
        "    temperature=0\n",
        ")\n",
        "print(\"Failure Case:\", response.choices[0].message.content.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lVDOCpB2sfu",
        "outputId": "a5eca416-dffb-4ac3-b3b8-b368eecb2e1f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failure Case: Comment ça va ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Chain of Thought (CoT)**\n",
        "Success Case:"
      ],
      "metadata": {
        "id": "e3FPkk0G3Ged"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CoT Example - Encourage step-by-step reasoning\n",
        "prompt = (\n",
        "    \"Q: If you have 2 apples and you buy 3 more, how many apples do you have in total?\\n\"\n",
        "    \"A: Let's think step by step.\"\n",
        ")\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "response = openai.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    max_tokens=50,\n",
        "    temperature=0\n",
        ")\n",
        "print(\"Success Case:\", response.choices[0].message.content.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzVfSoJ73ISV",
        "outputId": "6b411e98-176d-449d-d8fe-f317c495294b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success Case: Step 1: Start with 2 apples.\n",
            "Step 2: Buy 3 more apples.\n",
            "Step 3: Add the 3 apples to the 2 apples you already have.\n",
            "Step 4: 2 apples + 3 apples =\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Failure Case:"
      ],
      "metadata": {
        "id": "8B5FEik13Wfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CoT Failure - Skips reasoning steps due to insufficient guidance\n",
        "prompt = \"If you have 2 apples and buy 3 more, how many apples do you have?\"\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "response = openai.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    max_tokens=50,\n",
        "    temperature=0\n",
        ")\n",
        "print(\"Failure Case:\", response.choices[0].message.content.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYq3ue8B3Yns",
        "outputId": "a1ef3dc2-0978-4afc-c269-adb79c5c20e7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failure Case: You would have 5 apples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Iterative Chain of Thought (iCoT)**\n",
        "Success Case:"
      ],
      "metadata": {
        "id": "60jAfsuo3ew1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iCoT Example\n",
        "steps = [\n",
        "    \"Step 1: Solve the math problem 12 + 15.\",\n",
        "    \"Step 2: Double the result of Step 1.\",\n",
        "    \"Step 3: Add 10 to the result of Step 2.\",\n",
        "    \"Step 4: What is the final answer?\"\n",
        "]\n",
        "results = []\n",
        "for step in steps:\n",
        "    MODEL = \"gpt-3.5-turbo\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": step}],\n",
        "        max_tokens=50,\n",
        "        temperature=0\n",
        "    )\n",
        "    results.append(response.choices[0].message.content.strip())\n",
        "\n",
        "    print(f\"{step}: {response.choices[0].message.content.strip()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NW2Y8uE3hQ1",
        "outputId": "cdae0aac-4b5a-4b03-be56-62c42d198947"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Solve the math problem 12 + 15.: 12 + 15 = 27.\n",
            "Step 2: Double the result of Step 1.: Let's say the result of Step 1 was x. \n",
            "\n",
            "Step 2: Double the result of Step 1\n",
            "\n",
            "2x\n",
            "Step 3: Add 10 to the result of Step 2.: Let's say the result of Step 2 is 20. \n",
            "\n",
            "Adding 10 to 20 would give us:\n",
            "\n",
            "20 + 10 = 30\n",
            "\n",
            "So, the result of adding 10 to the result of Step 2 is 30\n",
            "Step 4: What is the final answer?: The final answer will depend on the specific question or problem that was being solved. Without that information, it is not possible to provide a final answer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Failure Case:"
      ],
      "metadata": {
        "id": "uft3O_5K48RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iCoT Failure (skips interdependencies between steps)\n",
        "steps = [\n",
        "    \"Solve 12 + 15.\",\n",
        "    \"Double the previous result.\",\n",
        "    \"Add 10 to the last result.\"\n",
        "]\n",
        "for step in steps:\n",
        "    MODEL = \"gpt-3.5-turbo\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": step}],\n",
        "        max_tokens=50,\n",
        "        temperature=0\n",
        "    )\n",
        "    print(f\"Failure Case: {step}: {response.choices[0].message.content.strip()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVVELcTd4-Er",
        "outputId": "762c0a94-d14d-4bef-ee16-2ebcfa474543"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failure Case: Solve 12 + 15.: 12 + 15 = 27\n",
            "Failure Case: Double the previous result.: If the previous result was x, then the new result would be 2x.\n",
            "Failure Case: Add 10 to the last result.: The last result was 20. Adding 10 to that gives us 30.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Tree of Thought (ToT)**\n",
        "Success Case:"
      ],
      "metadata": {
        "id": "ZLkgEl9R5kUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.7)\n",
        "template = PromptTemplate(\n",
        "    input_variables=['problem'],\n",
        "    template=\"\"\"\n",
        "    Let's solve this step by step and consider multiple possibilities:\n",
        "    {problem}\n",
        "    \"\"\"\n",
        ")\n",
        "chain = LLMChain(llm=llm, prompt=template)\n",
        "response = chain.run(problem=\"What are the factors of 28?\")\n",
        "print(\"Success Case:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW52ZJk55lSp",
        "outputId": "f90ec2f0-3f76-460c-fab5-75d6c796fe72"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-5ce43d1ff531>:9: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=template)\n",
            "<ipython-input-36-5ce43d1ff531>:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chain.run(problem=\"What are the factors of 28?\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success Case: 1. 1 x 28\n",
            "    2. 2 x 14\n",
            "    3. 4 x 7\n",
            "\n",
            "    So, there are three pairs of factors for 28. Now, let's check if any of these pairs are prime numbers.\n",
            "    1. 1 x 28 - 1 is not a prime number as it is a composite number.\n",
            "    2. 2 x 14 - 2 is a prime number.\n",
            "    3. 4 x 7 - 4 is not a prime number as it is a composite number.\n",
            "\n",
            "    Therefore, out of the three pairs of factors of 28, only one pair (2 x 14) consists of a prime number (2).\n",
            "\n",
            "    Hence, the prime factors of 28 are 2 and 14.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Failure Case:"
      ],
      "metadata": {
        "id": "nysh6QpT5qY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ToT Failure - Insufficient reasoning\n",
        "prompt = \"What are the factors of 28?\"\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "response = openai.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    max_tokens=50,\n",
        "    temperature=0\n",
        ")\n",
        "print(\"Failure Case:\", response.choices[0].message.content.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuN-xhRi5qCY",
        "outputId": "e3fbdbf6-8172-460a-d036-ed1448eb529d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failure Case: The factors of 28 are: 1, 2, 4, 7, 14, and 28.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Graph of Thought (GoT)**\n",
        "Success Case:"
      ],
      "metadata": {
        "id": "h4KFKy0-58w1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GoT Example - Interrelated solutions\n",
        "prompt = (\n",
        "    \"Solve these related problems:\\n\"\n",
        "    \"1. What is 12 + 8?\\n\"\n",
        "    \"2. What is 20 divided by the answer to 1?\\n\"\n",
        "    \"3. Multiply the answer to 2 by 4.\\n\"\n",
        "    \"Show all steps.\"\n",
        ")\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "response = openai.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    max_tokens=50,\n",
        "    temperature=0\n",
        ")\n",
        "print(\"Success Case:\", response.choices[0].message.content.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DGatyWi55uH",
        "outputId": "820aea56-360e-4663-bfb2-7e51e95dfb59"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success Case: 1. 12 + 8 = 20\n",
            "2. 20 divided by 20 = 1\n",
            "3. 1 x 4 = 4\n",
            "\n",
            "Therefore, the final answer is 4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Failure Case:"
      ],
      "metadata": {
        "id": "ijzolwmP6gUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GoT Failure - Lack of dependency tracking\n",
        "prompt = (\n",
        "    \"Solve these problems:\\n\"\n",
        "    \"1. Add 12 and 8.\\n\"\n",
        "    \"2. Divide 20 by 4.\\n\"\n",
        "    \"3. Multiply 5 by 3.\"\n",
        ")\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "response = openai.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    max_tokens=50,\n",
        "    temperature=0\n",
        ")\n",
        "print(\"Failure Case:\", response.choices[0].message.content.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRFv4Xpv6iUR",
        "outputId": "11bcbd38-b08e-402d-f34a-0599af7f7ae6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failure Case: 1. 12 + 8 = 20\n",
            "2. 20 ÷ 4 = 5\n",
            "3. 5 x 3 = 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Agent of Thought (AoT)**\n",
        "Success Case:"
      ],
      "metadata": {
        "id": "U-_S8xB76nmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulating an agent collaborating on tasks\n",
        "prompts = [\n",
        "    \"Agent A: Find the capital of France.\",\n",
        "    \"Agent B: Cross-reference the population of the city.\",\n",
        "    \"Agent A: Add historical context about its landmarks.\"\n",
        "]\n",
        "for prompt in prompts:\n",
        "    MODEL = \"gpt-3.5-turbo\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=50,\n",
        "        temperature=0\n",
        "    )\n",
        "    print(f\"{prompt}: {response.choices[0].message.content.strip()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjnJXg0J6qsX",
        "outputId": "fcf1514c-233b-4962-de6a-39caf32a594f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent A: Find the capital of France.: The capital of France is Paris.\n",
            "Agent B: Cross-reference the population of the city.: Agent A: The population of the city is approximately 500,000 according to the latest census data.\n",
            "Agent A: Add historical context about its landmarks.: The landmarks in this area hold significant historical importance, dating back to the early 19th century. The iconic clock tower was built in 1820 as a symbol of the town's prosperity during the Industrial Revolution. The grand cathedral, constructed in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. REACT (Reasoning and Acting)**\n",
        "Success Case:"
      ],
      "metadata": {
        "id": "g53dAl7a60V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REACT Example - Reasoning and acting in tandem\n",
        "prompt = (\n",
        "    \"Question: Who is the President of the United States?\\n\"\n",
        "    \"Thought: I need to retrieve the latest data.\\n\"\n",
        "    \"Action: Use external API to find the current President.\\n\"\n",
        "    \"Observation: The President is Joe Biden.\\n\"\n",
        "    \"Final Answer: Joe Biden.\"\n",
        ")\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "response = openai.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    max_tokens=50,\n",
        "    temperature=0\n",
        ")\n",
        "print(\"Success Case:\", response.choices[0].message.content.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_F5xIpd63B1",
        "outputId": "8f584c8e-b3ec-40d8-c130-b0af88fa4a8e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success Case: President Joe Biden is the current President of the United States.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. RASCEF**\n",
        "Success Case:"
      ],
      "metadata": {
        "id": "I0jBYLUE7N1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RASCEF Example - Building structured reasoning\n",
        "prompt = (\n",
        "    \"Problem: How do you determine if a number is prime?\\n\"\n",
        "    \"Reason: A number is prime if it has no divisors other than 1 and itself.\\n\"\n",
        "    \"Evidence: Check divisibility up to the square root of the number.\\n\"\n",
        "    \"Framework: Write a Python function to validate the reasoning.\\n\"\n",
        ")\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "response = openai.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    max_tokens=50,\n",
        "    temperature=0\n",
        ")\n",
        "print(\"Success Case:\", response.choices[0].message.content.strip())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2Q_Ey2J7NPU",
        "outputId": "1bb1ba02-675c-446b-af5f-9e3af8909f30"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success Case: ```python\n",
            "import math\n",
            "\n",
            "def is_prime(num):\n",
            "    if num <= 1:\n",
            "        return False\n",
            "    if num == 2:\n",
            "        return True\n",
            "    if num % 2 == 0:\n",
            "        return False\n",
            "    \n",
            "    for i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. Forest of Thoughts**\n",
        "Success Case:"
      ],
      "metadata": {
        "id": "pyo2oj8g7WJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forest of Thoughts using LangChain\n",
        "llm = OpenAI(temperature=0.7)\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Explore various solutions for: {topic}\"\n",
        ")\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "response = chain.run(topic=\"Climate Change Mitigation Strategies\")\n",
        "print(\"Success Case:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcNCdLAy7Y4u",
        "outputId": "4a3a1b50-ce91-41e6-d075-b4f3cdf46d67"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success Case: \n",
            "\n",
            "1. Renewable Energy Sources\n",
            "One key solution for mitigating climate change is to transition to renewable energy sources, such as solar, wind, hydropower, and geothermal energy. These sources emit little to no greenhouse gases and can replace fossil fuels in powering homes, businesses, and transportation.\n",
            "\n",
            "2. Energy Efficiency\n",
            "Improving energy efficiency in buildings, industries, and transportation can also significantly reduce greenhouse gas emissions. This can be achieved through measures such as better insulation, efficient lighting and appliances, and more fuel-efficient vehicles.\n",
            "\n",
            "3. Carbon Pricing\n",
            "Implementing a price on carbon emissions, either through a carbon tax or a cap-and-trade system, can incentivize industries and individuals to reduce their carbon footprint. This can also generate revenue that can be reinvested in clean energy and climate change adaptation measures.\n",
            "\n",
            "4. Sustainable Land Use Practices\n",
            "Deforestation and land use change are major contributors to climate change. Implementing sustainable land use practices, such as reforestation, agroforestry, and sustainable agriculture techniques, can help sequester carbon and reduce emissions.\n",
            "\n",
            "5. Carbon Capture and Storage\n",
            "Carbon capture and storage technologies can capture carbon dioxide emissions from power plants and industrial facilities and store them underground. This can help reduce emissions from these sources and mitigate their impact\n"
          ]
        }
      ]
    }
  ]
}